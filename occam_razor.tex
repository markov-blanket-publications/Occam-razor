\documentclass{article}

% ready for submission
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}}

\title{An ergodic approach to Occam's razor}

\date{March 4, 2021}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Aidan Rocke\\
  \texttt{aidanrocke@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
An algorithmic Occam's razor may be derived using the Expected Kolmogorov Complexity of a discrete ergodic variable. This derivation is based upon a combination of Bayesian and ergodic perspectives that are both necessary and sufficient in the context of the scientific method as it is applied in the natural sciences. 
\end{abstract}


\section{A Bayesian perspective on computation}

As computations are observer-dependent, computation is fundamentally Bayesian. In particular, the uncertainty associated with a discrete ergodic variable is defined with respect to a predictive model.
It follows that Occam's razor for a discrete ergodic variable $X$ is a measure of epistemic uncertainty or the memory
requirements of the ideal model for predicting the behaviour of $X$.

In this setting, the minimum description length of $X$ is given by the most parsimonious model $\Omega$ for
predicting the behaviour of $X$:

\begin{equation}
\mathbb{E}[K(X)] = \mathbb{E}[-\ln P(X\lvert \Omega)P(\Omega)] = H(X \lvert \Omega) + H(\Omega)
\end{equation}

where $H(\Omega)$ is the complexity of the model $\Omega$ and $H(X \lvert \Omega)$ is the expected information
gained by $\Omega$ from observing $X$. 

The reason why the expression in (1) has a probabilistic representation is that the behaviour of a discrete ergodic variable
is described by its probability distribution. From an ergodic perspective, these
probabilities also have a natural frequentist interpretation. 

\section{An ergodic analysis}

Given that an event that occurs with frequency $p$ generally requires modelling a sequence of length $\sim \frac{1}{p}$,
in order to encode the structure of such an event, a machine would generally need a number of bits proportional to:

\begin{equation}
\ln(\frac{1}{p}) = - \ln(p)
\end{equation}

But, how should we define the constant of proportionality? If we assume that the memory of the machine is finite and that
the data-generating process is ergodic, an optimal encoding would use the expected number of bits:

\begin{equation}
-p \cdot \ln(p)
\end{equation}

in order to encode an event that occurs with frequency $p$.

Regarding the assumptions, I may make a couple remarks. First, the ergodic assumption is equivalent to the premise that
scientific experiments are repeatable in the natural sciences. Second, any realisable digital computer has finite memory. 

\newpage 

\section{Application: Ockcam's razor and asymptotic incompressibility}

Given a binary sequence $X_N = \{x_i\}_{i=1}^N$, we say that $X_N$ is \textit{asymptotically incompressible} if given the subsequence $X_k$ and $N >>k$ 
on average we would not profit by gambling on the $N-k$ terms in $X_N$ based 
on the partial knowledge provided by $X_k$. If $X_N$ satisfies these assumptions then Occam's razor applied to $X_N$ scales as follows: 

\begin{equation}
\mathbb{E}[K(X_N)] \sim N	
\end{equation}

which means that the average size(in bits) of the smallest approximately correct predictive model, found using machine learning methods, scales with $N$. It follows that an effective betting strategy has infinite sample complexity and therefore such a strategy is not learnable.  

\section{Discussion}

I would like to point out that mainstream Algorithmic Information Theory insists that the Expected Kolmogorov Complexity
of a discrete ergodic variable equals its Shannon entropy:

\begin{equation}
\mathbb{E}[K(X)] = H(X)
\end{equation}

by appealing to Solomonoff's Universal distribution which may address the No Free Lunch problem [1,2]. What we may infer from this is that the mainstream theory is both incomplete. 

The implicit error in (5) is to assume that local computations are observer-independent. Since we can't remove the observer in (1),
we have:

\begin{equation}
X = \Omega \implies \mathbb{E}[K(\Omega)] = H(\Omega)
\end{equation}

So information that is truly incompressible is self-referential.

\section*{References}

\small
[1] Peter Grünwald and Paul Vitányi. Shannon Information and Kolmogorov Complexity. 2010.

[2] Tom Everitt, Tor Lattimore, Marcus Hutter. Free Lunch for Optimisation under the Universal Distribution. 2016.

[3] Schnorr, C. P. (1971). "A unified approach to the definition of a random sequence". Mathematical Systems Theory.

[4] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press. 2014. 

\end{document}

\end{document}