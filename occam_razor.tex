\documentclass{article}

% ready for submission
\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}}

\title{An ergodic approach to Occam's razor}

\date{March 4, 2021}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Aidan Rocke\\
  \texttt{aidanrocke@gmail.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
An algorithmic Occam's razor may be derived using the Expected Kolmogorov Complexity of a discrete random variable. This derivation is based upon a combination of Bayesian and ergodic perspectives that are both necessary and sufficient in the context of the scientific method as it is applied in the natural sciences. 
\end{abstract}


\section{A Bayesian perspective on computation}

As computations are observer-dependent, computation is fundamentally Bayesian. In particular, the uncertainty associated with a discrete random variable is defined with respect to a predictive model.
It follows that Occam's razor for a discrete random variable $X$ is a measure of epistemic uncertainty or the memory
requirements of the ideal model for predicting the behaviour of $X$.

In this setting, the minimum description length of $X$ is given by the most parsimonious model $\Omega$ for
predicting the behaviour of $X$:

\begin{equation}
\mathbb{E}[K(X)] = \mathbb{E}[-\ln P(X\lvert \Omega)P(\Omega) = H(X \lvert \Omega) + H(\Omega)
\end{equation}

where $H(\Omega)$ is the complexity of the model $\Omega$ and $H(X \lvert \Omega)$ is the expected information
gained by $\Omega$ from observing $X$. 

The reason why the expression in (1) has a probabilistic representation is that the behaviour of a discrete random variable
is described by its probability distribution. From an ergodic perspective, these
probabilities also have a natural frequentist interpretation. 

\section{An ergodic analysis}

Given that an event that occurs with frequency $p$ generally requires modelling a sequence of length $\sim \frac{1}{p}$,
in order to encode the structure of such an event, a machine would generally need a number of bits proportional to:

\begin{equation}
\ln(\frac{1}{p}) = - \ln(p)
\end{equation}

But, how should we define the constant of proportionality? If we assume that the memory of the machine is finite and that
the data-generating process is ergodic, an optimal encoding would use the expected number of bits:

\begin{equation}
-p \cdot \ln(p)
\end{equation}

in order to encode an event that occurs with frequency $p$.

Regarding the assumptions, I may make a couple remarks. First, the ergodic assumption is equivalent to the premise that
scientific experiments are repeatable in the natural sciences. As for finite memory, all Turing machines have finite
memory.

\newpage 

\section{Discussion}

I would like to point out that mainstream Algorithmic Information Theory insists that the Expected Kolmogorov Complexity
of a discrete random variable equals its Shannon entropy:

\begin{equation}
\mathbb{E}[K(X)] = H(X)
\end{equation}

by appealing to contrived mathematical notions such 'Universal distributions' which may 'solve' the No Free Lunch problem [1,2].

The same AIT researchers complain that there aren't many scientists using Algorithmic Information Theory for predictive
modelling.

\section*{References}

\small
[1] Peter Grünwald and Paul Vitányi. Shannon Information and Kolmogorov Complexity. 2010.

[2] Tom Everitt, Tor Lattimore, Marcus Hutter. Free Lunch for Optimisation under the Universal Distribution. 2016.


\end{document}

\end{document}